{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm.factory import LLMInterface\n",
    "from setting.db import SessionLocal\n",
    "from llm.embedding import get_text_embedding\n",
    "\n",
    "# llm_client = LLMInterface(\"bedrock\", \"us.deepseek.r1-v1:0\")\n",
    "llm_client = LLMInterface(\"bedrock\", \"arn:aws:bedrock:us-east-1:841162690310:inference-profile/us.anthropic.claude-3-7-sonnet-20250219-v1:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Define the path to the JSON configuration file\n",
    "config_file_path = 'docs/business_operations/business_operations_docs.json'\n",
    "\n",
    "# Variable to store the loaded data\n",
    "loaded_docs = []\n",
    "\n",
    "# Read the JSON configuration file\n",
    "try:\n",
    "    with open(config_file_path, 'r', encoding='utf-8') as f:\n",
    "        loaded_docs = json.load(f)\n",
    "    print(f\"Successfully loaded configuration from: {config_file_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Configuration file not found at '{config_file_path}'\")\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: Could not decode JSON from file '{config_file_path}'. Check file format.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred while reading the file: {e}\")\n",
    "\n",
    "if len(loaded_docs) > 0:\n",
    "    print(\"\\nExample: Accessing first document data:\")\n",
    "    print(loaded_docs[0])\n",
    "else:\n",
    "    print(\"\\nConfiguration file is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from knowledge_graph.knowledge import KnowledgeBuilder\n",
    "\n",
    "success_index = []\n",
    "kb_builder = KnowledgeBuilder(llm_client, get_text_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "for index, doc in enumerate(loaded_docs):\n",
    "    if index in success_index:\n",
    "        continue\n",
    "    print(doc['path'])\n",
    "    try:\n",
    "        kb_builder.extract_knowledge_blocks(\n",
    "            doc['path'], \n",
    "            doc['metadata']\n",
    "        )\n",
    "        success_index.append(index)\n",
    "        time.sleep(60)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"process index {index} failed, {e}\", exc_info=True)\n",
    "        time.sleep(120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write contextual information into Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from knowledge_graph.models import KnowledgeBlock, SourceData\n",
    "from setting.db import SessionLocal\n",
    "from llm.embedding import get_text_embedding\n",
    "\n",
    "\n",
    "query_vector = get_text_embedding(\"How to update protobuf message definition\")\n",
    "\n",
    "with SessionLocal() as db:\n",
    "    # Join KnowledgeBlock with SourceData and select desired fields\n",
    "    results = db.query(\n",
    "        KnowledgeBlock.name.label(\"block_name\"), # Alias to avoid name collision\n",
    "        KnowledgeBlock.context,\n",
    "        KnowledgeBlock.content,\n",
    "        KnowledgeBlock.source_id,\n",
    "        KnowledgeBlock.position_in_source,\n",
    "        # KnowledgeBlock.source_version, # Get version from SourceData instead\n",
    "        SourceData.name.label(\"source_name\"),\n",
    "        SourceData.link.label(\"source_link\"),\n",
    "        SourceData.version.label(\"source_version\"),\n",
    "        SourceData.data_type.label(\"source_data_type\"),\n",
    "    ).join(SourceData, KnowledgeBlock.source_id == SourceData.id).all()\n",
    "    \n",
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexual_docs = {}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row['source_link'] not in contexual_docs:\n",
    "        contexual_docs[row['source_link']] = {\n",
    "            \"source_name\": row['source_name'],\n",
    "            \"content\": []\n",
    "        }\n",
    "\n",
    "    contexual_docs[row['source_link']]['content'].append({\n",
    "        \"position\": row['position_in_source'],\n",
    "        \"context\": row['context'],\n",
    "        \"content\": row['content']\n",
    "    })\n",
    "    \n",
    "len(contexual_docs.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Function to sanitize filenames\n",
    "def sanitize_filename(name):\n",
    "    # Remove invalid characters\n",
    "    name = re.sub(r'[\\\\/*?:\"<>|]', \"\", name)\n",
    "    # Replace spaces with underscores\n",
    "    name = name.replace(\" \", \"_\")\n",
    "    return name + \".txt\"\n",
    "\n",
    "# Define the output directory\n",
    "output_dir = \"contexual_docs\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Iterate through the contextual documents\n",
    "for source_link, data in contexual_docs.items():\n",
    "    source_name = data['source_name']\n",
    "    content_list = data['content']\n",
    "\n",
    "    # Sanitize the source name for the filename\n",
    "    filename = sanitize_filename(source_name)\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "    # Sort the content list by position\n",
    "    content_list.sort(key=lambda x: x['position'])\n",
    "\n",
    "    # Build the file content\n",
    "    file_content_parts = []\n",
    "    for item in content_list:\n",
    "        context_str = f\"<context>{item['context']}</context>\"\n",
    "        content_str = item['content']\n",
    "        file_content_parts.append(f\"{context_str}\\n{content_str}\")\n",
    "\n",
    "    # Join parts with 5 newlines\n",
    "    file_content = \"\\n######\\n\".join(file_content_parts)\n",
    "\n",
    "    # Write the content to the file\n",
    "    try:\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(file_content)\n",
    "        print(f\"Successfully wrote: {filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing file {filepath}: {e}\")\n",
    "\n",
    "print(f\"\\nFinished writing files to the '{output_dir}' directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
