{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm.factory import LLMInterface\n",
    "from setting.db import SessionLocal\n",
    "from llm.embedding import get_text_embedding\n",
    "\n",
    "# llm_client = LLMInterface(\"bedrock\", \"us.deepseek.r1-v1:0\")\n",
    "llm_client = LLMInterface(\"bedrock\", \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Define the path to the JSON configuration file\n",
    "config_file_path = 'docs/business_operations/business_operations_docs.json'\n",
    "\n",
    "# Variable to store the loaded data\n",
    "loaded_docs = []\n",
    "\n",
    "# Read the JSON configuration file\n",
    "try:\n",
    "    with open(config_file_path, 'r', encoding='utf-8') as f:\n",
    "        loaded_docs = json.load(f)\n",
    "    print(f\"Successfully loaded configuration from: {config_file_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Configuration file not found at '{config_file_path}'\")\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: Could not decode JSON from file '{config_file_path}'. Check file format.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred while reading the file: {e}\")\n",
    "\n",
    "if len(loaded_docs) > 0:\n",
    "    print(\"\\nExample: Accessing first document data:\")\n",
    "    print(loaded_docs[0])\n",
    "else:\n",
    "    print(\"\\nConfiguration file is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from knowledge_graph.knowledge import KnowledgeBuilder\n",
    "\n",
    "success_index = []\n",
    "kb_builder = KnowledgeBuilder(llm_client, get_text_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "for index, doc in enumerate(loaded_docs):\n",
    "    if index in success_index:\n",
    "        continue\n",
    "    print(doc['path'])\n",
    "    try:\n",
    "        kb_builder.extract_knowledge_blocks(\n",
    "            doc['path'], \n",
    "            doc['metadata']\n",
    "        )\n",
    "        success_index.append(index)\n",
    "        time.sleep(60)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"process index {index} failed, {e}\", exc_info=True)\n",
    "        time.sleep(120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write contextual information into Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from knowledge_graph.models import KnowledgeBlock, SourceData\n",
    "from setting.db import SessionLocal\n",
    "from llm.embedding import get_text_embedding\n",
    "\n",
    "\n",
    "query_vector = get_text_embedding(\"How to update protobuf message definition\")\n",
    "\n",
    "with SessionLocal() as db:\n",
    "    # Join KnowledgeBlock with SourceData and select desired fields\n",
    "    results = db.query(\n",
    "        KnowledgeBlock.name.label(\"block_name\"), # Alias to avoid name collision\n",
    "        KnowledgeBlock.context,\n",
    "        KnowledgeBlock.content,\n",
    "        KnowledgeBlock.source_id,\n",
    "        KnowledgeBlock.position_in_source,\n",
    "        # KnowledgeBlock.source_version, # Get version from SourceData instead\n",
    "        SourceData.name.label(\"source_name\"),\n",
    "        SourceData.link.label(\"source_link\"),\n",
    "        SourceData.version.label(\"source_version\"),\n",
    "        SourceData.data_type.label(\"source_data_type\"),\n",
    "    ).join(SourceData, KnowledgeBlock.source_id == SourceData.id).all()\n",
    "    \n",
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexual_docs = {}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row['source_link'] not in contexual_docs:\n",
    "        contexual_docs[row['source_link']] = {\n",
    "            \"source_name\": row['source_name'],\n",
    "            \"source_link\": row['source_link'],\n",
    "            \"source_version\": row['source_version'],\n",
    "            \"content\": []\n",
    "        }\n",
    "\n",
    "    contexual_docs[row['source_link']]['content'].append({\n",
    "        \"position\": row['position_in_source'],\n",
    "        \"context\": row['context'],\n",
    "        \"content\": row['content']\n",
    "    })\n",
    "    \n",
    "len(contexual_docs.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Function to sanitize filenames\n",
    "def sanitize_filename(name):\n",
    "    # Remove invalid characters\n",
    "    name = re.sub(r'[\\\\/*?:\"<>|]', \"\", name)\n",
    "    # Replace spaces with underscores\n",
    "    # name = name.replace(\" \", \"_\")\n",
    "    return name + \".txt\"\n",
    "\n",
    "# Define the output directory\n",
    "output_dir = \"contexual_docs\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Iterate through the contextual documents\n",
    "for source_link, data in contexual_docs.items():\n",
    "    source_name = data['source_name']\n",
    "    source_link = data['source_link']\n",
    "    source_version = data['source_version']\n",
    "    content_list = data['content']\n",
    "\n",
    "    # Sanitize the source name for the filename\n",
    "    filename = sanitize_filename(source_name)\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "    file_info = f\"<doc_link>[{source_name}]({source_link})</doc_link>\\n<doc_version>{source_version}</doc_version>\"\n",
    "\n",
    "    # Sort the content list by position\n",
    "    content_list.sort(key=lambda x: x['position'])\n",
    "\n",
    "    # Build the file content\n",
    "    file_content_parts = []\n",
    "    for item in content_list:\n",
    "        context_str = f\"<context>{item['context']}</context>\"\n",
    "        content_str = item['content']\n",
    "        file_content_parts.append(f\"{file_info}\\n\\n{context_str}\\n\\n{content_str}\")\n",
    "\n",
    "    # Join parts with 5 newlines\n",
    "    file_content = \"\\n######\\n\".join(file_content_parts)\n",
    "\n",
    "    # Write the content to the file\n",
    "    try:\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(file_content)\n",
    "        print(f\"Successfully wrote: {filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing file {filepath}: {e}\")\n",
    "\n",
    "print(f\"\\nFinished writing files to the '{output_dir}' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Graph into files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "\n",
    "from setting.db import SessionLocal\n",
    "\n",
    "graph_entity_df = None\n",
    "graph_relationship_df = None\n",
    "with SessionLocal() as db:\n",
    "    entity_query = \"\"\"SELECT\n",
    "  c.name AS concept_name,\n",
    "  c.definition AS concept_definition,\n",
    "  c.version AS concept_version,\n",
    "  s.name AS source_name,\n",
    "  s.version AS source_version,\n",
    "  s.link AS source_link\n",
    "FROM concepts c\n",
    "LEFT JOIN relationships r ON r.source_id = c.id AND r.relationship_type = 'SOURCE_OF'\n",
    "LEFT JOIN source_data s ON r.target_id = s.id;\n",
    "\"\"\"\n",
    "    relationships_query = \"\"\"select \n",
    "    l.name as source_name,\n",
    "    l.definition as source_definition,\n",
    "    rel.relationship_desc as relationship_description,\n",
    "    r.name as target_name,\n",
    "    r.definition as target_definition\n",
    "from relationships rel\n",
    "left join concepts l on l.id = rel.source_id\n",
    "left join concepts r on r.id = rel.target_id\n",
    "where rel.source_type = 'Concept' and rel.target_type = 'Concept';\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "try:\n",
    "    # Use the session's underlying connection (engine) with pandas\n",
    "    with SessionLocal() as db:\n",
    "        # db.bind gives access to the engine/connection pandas needs\n",
    "        graph_entity_df = pd.read_sql_query(sql=text(entity_query), con=db.bind)\n",
    "        graph_relationship_df = pd.read_sql_query(sql=text(relationships_query), con=db.bind)\n",
    "\n",
    "    print(\"Graph data loaded successfully from database using raw SQL.\")\n",
    "    # print(graph_entity_df.head()) # Optional: display first few rows\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_entity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_relationship_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts_map = {}\n",
    "\n",
    "for index, row in graph_entity_df.iterrows():\n",
    "    concept_key = row['concept_name'] + row['concept_definition']\n",
    "    source_key = row['source_link']\n",
    "\n",
    "    if concept_key in concepts_map:\n",
    "        concept_sources = concepts_map[concept_key].get(\"sources\", None)\n",
    "        if concept_sources and source_key not in concept_sources:\n",
    "            concepts_map[concept_key][\"sources\"][source_key] = {\n",
    "                \"name\": row['source_name'],\n",
    "                'link': row['source_link'],\n",
    "                \"source_version\": row['source_version'],\n",
    "            }\n",
    "    else:\n",
    "        concepts_map[concept_key] = {\n",
    "            \"name\": row['concept_name'],\n",
    "            \"definition\": row['concept_definition'],\n",
    "            \"concept_version\": row[\"concept_version\"],\n",
    "            \"sources\": {\n",
    "                source_key: {\n",
    "                    \"name\": row['source_name'],\n",
    "                    'link': row['source_link'],\n",
    "                    \"source_version\": row['source_version'],\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "relationship_map = {}\n",
    "for index, row in graph_relationship_df.iterrows():\n",
    "    source_key = row['source_name'] + row['source_definition']\n",
    "    target_key = row['target_name'] + row['target_definition']\n",
    "\n",
    "    if source_key in relationship_map:\n",
    "        relationship_map[source_key].append({\n",
    "            \"source\": row['source_name'],\n",
    "            \"relationship\": row['relationship_description'],\n",
    "            \"target\": row['target_name']\n",
    "        })\n",
    "    else:\n",
    "         relationship_map[source_key] = [{\n",
    "                \"source\": row['source_name'],\n",
    "                \"relationship\": row['relationship_description'],\n",
    "                \"target\": row['target_name']\n",
    "        }]\n",
    "         \n",
    "    if target_key in relationship_map:\n",
    "        relationship_map[target_key].append({\n",
    "            \"source\": row['source_name'],\n",
    "            \"relationship\": row['relationship_description'],\n",
    "            \"target\": row['target_name']\n",
    "        })\n",
    "    else:\n",
    "         relationship_map[target_key] = [{\n",
    "                \"source\": row['source_name'],\n",
    "                \"relationship\": row['relationship_description'],\n",
    "                \"target\": row['target_name']\n",
    "        }]\n",
    "\n",
    "\n",
    "\n",
    "for key, concept in concepts_map.items():\n",
    "    if key in relationship_map:\n",
    "        concepts_map[key]['relationships'] = relationship_map[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_df = pd.DataFrame(concepts_map.values())\n",
    "graph_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def write_graph_to_markdown(df, filename=\"graph_output.md\"):\n",
    "    \"\"\"\n",
    "    Writes the graph data from a DataFrame to a markdown file.\n",
    "\n",
    "    Each row in the DataFrame is formatted as a section in the markdown file.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing graph entities.\n",
    "                           Expected columns: 'name', 'definition', 'concept_version',\n",
    "                           'sources' (dict), 'relationships' (list of dicts).\n",
    "        filename (str): The name of the output markdown file. Default is \"graph_output.md\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            for index, row in df.iterrows():\n",
    "                # Validate required columns exist and handle potential missing data\n",
    "                concept_name = row.get('name', f'Unnamed Entity {index+1}')\n",
    "                concept_definition = row.get('definition', 'No definition provided.')\n",
    "                concept_version = row.get('concept_version', 'N/A')\n",
    "                relationships = row.get('relationships', [])\n",
    "                sources = row.get('sources', {})\n",
    "\n",
    "                # Entity Name (Level 2 Heading)\n",
    "                f.write(f\"## {concept_name}\\n\\n\")\n",
    "\n",
    "                # Definition Section (Level 3 Heading)\n",
    "                f.write(f\"### concept definition\\n\\n\")\n",
    "                f.write(f\"{concept_definition}\\n\\n\")\n",
    "\n",
    "                # Concept Version\n",
    "                f.write(f\"concept version: {concept_version}\\n\\n\")\n",
    "\n",
    "                # Relationships Section (Level 2 Heading)\n",
    "                f.write(f\"### relationships\\n\\n\")\n",
    "                if isinstance(relationships, list) and relationships:\n",
    "                    for rel in relationships:\n",
    "                        # Use .get() for safer access to dictionary keys\n",
    "                        source = rel.get('source', 'Unknown Source')\n",
    "                        relationship = rel.get('relationship', 'Unknown Relationship')\n",
    "                        target = rel.get('target', 'Unknown Target')\n",
    "                        f.write(f\"- {source} -> {relationship} -> {target}\\n\")\n",
    "                    f.write(\"\\n\") # Add a newline after the list\n",
    "                else:\n",
    "                    f.write(\"No relationships defined.\\n\\n\")\n",
    "\n",
    "                # References Section (Level 1 Heading)\n",
    "                f.write(f\"### references\\n\\n\")\n",
    "                if isinstance(sources, dict) and sources:\n",
    "                    # Iterate through the source information dictionaries (values of the main dict)\n",
    "                    for key, src_info in sources.items():\n",
    "                        if isinstance(src_info, dict):\n",
    "                            # Use .get() for safer access to dictionary keys\n",
    "                            name = src_info.get('name', 'Unknown Source Name')\n",
    "                            link = src_info.get('link', '#') # Default to '#' if no link\n",
    "                            version = src_info.get('source_version', 'N/A')\n",
    "                            f.write(f\"- [{name}]({link}). version: {version}\\n\")\n",
    "                    f.write(\"\\n\") # Add a newline after the list\n",
    "                else:\n",
    "                    f.write(\"No sources defined.\\n\\n\")\n",
    "\n",
    "                # Add a separator between entries for readability, except for the last one\n",
    "                if index < len(df) - 1:\n",
    "                    f.write(\"######\\n\\n\")\n",
    "\n",
    "        print(f\"Successfully wrote graph data to {filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "write_graph_to_markdown(graph_df, \"business_operation_graph.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
