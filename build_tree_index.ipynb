{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setting.db import SessionLocal\n",
    "\n",
    "from llm.factory import LLMInterface\n",
    "from llm.embedding import get_text_embedding\n",
    "from knowledge_graph.knowledge import KnowledgeBuilder\n",
    "\n",
    "\n",
    "llm_client = LLMInterface(\"bedrock\", \"arn:aws:bedrock:us-east-1:841162690310:inference-profile/us.anthropic.claude-3-7-sonnet-20250219-v1:0\")\n",
    "kb_builder = KnowledgeBuilder(llm_client, get_text_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I\\'ve analyzed the provided documents related to \"Compensation Metrics\" and will create a knowledge graph based on the content. Let me begin with my analysis:\\n\\n```json\\n{\\n  \"entities\": [\\n    {\\n      \"name\": \"Compensation Metrics\",\\n      \"definition\": \"Specific business measurements used to calculate, track, and evaluate sales compensation performance. These metrics include Brand New Cloud Customer, High-Quality OP, SKA/KA segmentation, ATR (Available to Renew), and Net ARR, which are used to determine quota attainment, commission rates, and bonus calculations within the PingCAP compensation structure.\"\\n    },\\n    {\\n      \"name\": \"Brand New Cloud Customer\",\\n      \"definition\": \"A customer classification metric defined as an organization with no Cloud Payment before FY25, with specific exceptions for whitelisted customers. To qualify as a Cloud Customer, the organization must achieve a Quarterly Ending ARR of at least $100K. This metric is used in the Cloud First Accelerator program, which provides an extra 20% commission for TiDB Cloud ACV multiplied by the Base Rate when sales representatives achieve their target number of Brand New Cloud Customers.\"\\n    },\\n    {\\n      \"name\": \"High-Quality OP\",\\n      \"definition\": \"A qualification metric for TiDB Enterprise Subscriptions to be eligible for compensation. A subscription meets High-Quality OP criteria if it is either categorized as SKA/KA, or if it is a non-KA with accumulated ACV in FY25 greater than or equal to $200k and deployment is in a public cloud (AWS/GCP/Azure/Alicloud/Tencent Cloud). Only subscriptions meeting these criteria will be compensated, with special approval reserved from Regional GM, Ops and CEO for deals unqualified for High-Quality OP.\"\\n    },\\n    {\\n      \"name\": \"SKA/KA Segmentation\",\\n      \"definition\": \"A customer classification system with two tiers: Strategic Key Account (SKA) and Key Account (KA). SKA criteria includes either ($500M+ Business or 1000+ Employees) AND unified ICP industry, OR $500K+ ARR with PingCAP. KA criteria includes either ($250M+ Business or 500+ Employees) AND unified ICP industry, OR $250K+ ARR with PingCAP. This segmentation affects qualification for High-Quality OP criteria.\"\\n    },\\n    {\\n      \"name\": \"ATR\",\\n      \"definition\": \"Available to Renew, defined as the ACV (Annual Contract Value) of all renewable orders with Service End Dates that fall within FY25. For Early Renewal of FY26 ATR booked in FY25, commission is based on FY26 ATR multiplied by FY25 Renewal PCR (Personal Commission Rate). No quota attainment is applied. If a renewal includes expansion, the expansion portion can be counted into Bucket 1. FY26 ATR will be refreshed after early renewal booking.\"\\n    },\\n    {\\n      \"name\": \"Cloud First Accelerator\",\\n      \"definition\": \"An incentive program that provides an extra 20% commission for TiDB Cloud ACV multiplied by the Base Rate when sales representatives achieve their target number of Brand New Cloud Customers. This accelerator is applied to Bucket 1 (New & Expansion ACV) compensation calculations to encourage sales representatives to prioritize cloud customer acquisition.\"\\n    },\\n    {\\n      \"name\": \"Compensation Buckets\",\\n      \"definition\": \"A three-part structure for organizing sales compensation targets and calculations in the PingCAP compensation plan. The structure consists of Bucket 1 (New & Expansion ACV), Bucket 2 (Renewal ACV), and Bucket 3 (MBO). Each bucket has its own quota, commission rates, and calculation methodologies. Bucket 1 includes an acceleration structure with increasing commission rates for exceeding quota, while Bucket 2 has a flat commission rate capped at 100% quota attainment.\"\\n    },\\n    {\\n      \"name\": \"Multi-year Orders Compensation\",\\n      \"definition\": \"A specialized compensation structure for deals spanning multiple years, divided into two main categories: Multi-year New and Expansion Orders, and Multi-year Renewal Orders. For Multi-year New and Expansion Orders, sales representatives receive Bucket 1 Target Bonus on Quota Attainment for the 1st year, plus 2% of ACV for both the 2nd and 3rd years. For Multi-year Renewal Orders, representatives receive Bucket 2 Target Bonus for the 1st renewal year, plus 2% of ACV for both the 2nd and 3rd years. Additional rules apply for Multi-year Cloud Commitment Plans regarding exceeded usage.\"\\n    }\\n  ],\\n  \"relationships\": [\\n    {\\n      \"source_entity\": \"Brand New Cloud Customer\",\\n      \"target_entity\": \"Cloud First Accelerator\",\\n      \"type\": \"triggers\",\\n      \"definition\": \"When a sales representative achieves their target number of Brand New Cloud Customers, it triggers the Cloud First Accelerator, which provides an extra 20% commission for TiDB Cloud ACV multiplied by the Base Rate. The Brand New Cloud Customer metric serves as the qualification threshold that must be met to activate this additional compensation incentive.\"\\n    },\\n    {\\n      \"source_entity\": \"High-Quality OP\",\\n      \"target_entity\": \"Compensation Metrics\",\\n      \"type\": \"qualifies\",\\n      \"definition\": \"The High-Quality OP criteria determine which TiDB Enterprise Subscriptions qualify for compensation. As stated in the compensation plan, \\'Only TiDB Enterprise Subscriptions that meet the High-Quality (KA/SKA) OP (On-Opportunity) criteria will be eligible for compensation; those that do not meet these criteria will not be compensated.\\' This creates a qualifying relationship where the High-Quality OP metric directly impacts compensation eligibility.\"\\n    },\\n    {\\n      \"source_entity\": \"SKA/KA Segmentation\",\\n      \"target_entity\": \"High-Quality OP\",\\n      \"type\": \"contributes_to\",\\n      \"definition\": \"The SKA/KA Segmentation directly contributes to determining High-Quality OP status, as being classified as either a Strategic Key Account (SKA) or Key Account (KA) is one of the two possible criteria for meeting High-Quality OP requirements. This relationship establishes that customer segmentation is a pathway to qualification for compensation eligibility.\"\\n    },\\n    {\\n      \"source_entity\": \"ATR\",\\n      \"target_entity\": \"Compensation Buckets\",\\n      \"type\": \"influences\",\\n      \"definition\": \"The ATR (Available to Renew) metric influences the Compensation Buckets by determining how renewal deals are calculated and allocated. Specifically, \\'Renewal ACV exceeding ATR will be calculated into Bucket 1\\' as stated in the compensation plan. Additionally, ATR defines the pool of renewable orders that can contribute to Bucket 2 quota attainment, directly affecting commission calculations.\"\\n    },\\n    {\\n      \"source_entity\": \"Cloud First Accelerator\",\\n      \"target_entity\": \"Compensation Buckets\",\\n      \"type\": \"enhances\",\\n      \"definition\": \"The Cloud First Accelerator enhances the Compensation Buckets structure by providing an additional 20% commission specifically for Bucket 1 (New & Expansion ACV) when the sales representative achieves their target number of Brand New Cloud Customers. This relationship represents how the accelerator program augments the standard bucket-based compensation calculation to incentivize cloud customer acquisition.\"\\n    },\\n    {\\n      \"source_entity\": \"Compensation Metrics\",\\n      \"target_entity\": \"Multi-year Orders Compensation\",\\n      \"type\": \"informs\",\\n      \"definition\": \"Compensation Metrics inform the Multi-year Orders Compensation structure by providing the foundational measurements and calculations upon which multi-year commission payments are based. The metrics determine how ACV is calculated, which deals qualify for compensation, and how commission rates are applied across multiple years of a contract, directly affecting the compensation outcomes for sales representatives with multi-year deals.\"\\n    }\\n  ]\\n}\\n```'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb = kb_builder.extract_knowledge_index(\n",
    "    \"docs/bo_tree/business_operation_knowledge_tree.mm\",\n",
    "    {\n",
    "        \"doc_version\": 1.0,\n",
    "        \"doc_link\": \"https://pingcap.feishu.cn/wiki/FYsKwV2p4iDrAxkEpPyc2o7enBb#mindmap\"\n",
    "    }\n",
    ")\n",
    "kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've analyzed the provided documents related to \"Compensation Metrics\" and will create a knowledge graph based on the content. Let me begin with my analysis:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"name\": \"Compensation Metrics\",\n",
      "      \"definition\": \"Specific business measurements used to calculate, track, and evaluate sales compensation performance. These metrics include Brand New Cloud Customer, High-Quality OP, SKA/KA segmentation, ATR (Available to Renew), and Net ARR, which are used to determine quota attainment, commission rates, and bonus calculations within the PingCAP compensation structure.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Brand New Cloud Customer\",\n",
      "      \"definition\": \"A customer classification metric defined as an organization with no Cloud Payment before FY25, with specific exceptions for whitelisted customers. To qualify as a Cloud Customer, the organization must achieve a Quarterly Ending ARR of at least $100K. This metric is used in the Cloud First Accelerator program, which provides an extra 20% commission for TiDB Cloud ACV multiplied by the Base Rate when sales representatives achieve their target number of Brand New Cloud Customers.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"High-Quality OP\",\n",
      "      \"definition\": \"A qualification metric for TiDB Enterprise Subscriptions to be eligible for compensation. A subscription meets High-Quality OP criteria if it is either categorized as SKA/KA, or if it is a non-KA with accumulated ACV in FY25 greater than or equal to $200k and deployment is in a public cloud (AWS/GCP/Azure/Alicloud/Tencent Cloud). Only subscriptions meeting these criteria will be compensated, with special approval reserved from Regional GM, Ops and CEO for deals unqualified for High-Quality OP.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"SKA/KA Segmentation\",\n",
      "      \"definition\": \"A customer classification system with two tiers: Strategic Key Account (SKA) and Key Account (KA). SKA criteria includes either ($500M+ Business or 1000+ Employees) AND unified ICP industry, OR $500K+ ARR with PingCAP. KA criteria includes either ($250M+ Business or 500+ Employees) AND unified ICP industry, OR $250K+ ARR with PingCAP. This segmentation affects qualification for High-Quality OP criteria.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"ATR\",\n",
      "      \"definition\": \"Available to Renew, defined as the ACV (Annual Contract Value) of all renewable orders with Service End Dates that fall within FY25. For Early Renewal of FY26 ATR booked in FY25, commission is based on FY26 ATR multiplied by FY25 Renewal PCR (Personal Commission Rate). No quota attainment is applied. If a renewal includes expansion, the expansion portion can be counted into Bucket 1. FY26 ATR will be refreshed after early renewal booking.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Cloud First Accelerator\",\n",
      "      \"definition\": \"An incentive program that provides an extra 20% commission for TiDB Cloud ACV multiplied by the Base Rate when sales representatives achieve their target number of Brand New Cloud Customers. This accelerator is applied to Bucket 1 (New & Expansion ACV) compensation calculations to encourage sales representatives to prioritize cloud customer acquisition.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Compensation Buckets\",\n",
      "      \"definition\": \"A three-part structure for organizing sales compensation targets and calculations in the PingCAP compensation plan. The structure consists of Bucket 1 (New & Expansion ACV), Bucket 2 (Renewal ACV), and Bucket 3 (MBO). Each bucket has its own quota, commission rates, and calculation methodologies. Bucket 1 includes an acceleration structure with increasing commission rates for exceeding quota, while Bucket 2 has a flat commission rate capped at 100% quota attainment.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Multi-year Orders Compensation\",\n",
      "      \"definition\": \"A specialized compensation structure for deals spanning multiple years, divided into two main categories: Multi-year New and Expansion Orders, and Multi-year Renewal Orders. For Multi-year New and Expansion Orders, sales representatives receive Bucket 1 Target Bonus on Quota Attainment for the 1st year, plus 2% of ACV for both the 2nd and 3rd years. For Multi-year Renewal Orders, representatives receive Bucket 2 Target Bonus for the 1st renewal year, plus 2% of ACV for both the 2nd and 3rd years. Additional rules apply for Multi-year Cloud Commitment Plans regarding exceeded usage.\"\n",
      "    }\n",
      "  ],\n",
      "  \"relationships\": [\n",
      "    {\n",
      "      \"source_entity\": \"Brand New Cloud Customer\",\n",
      "      \"target_entity\": \"Cloud First Accelerator\",\n",
      "      \"type\": \"triggers\",\n",
      "      \"definition\": \"When a sales representative achieves their target number of Brand New Cloud Customers, it triggers the Cloud First Accelerator, which provides an extra 20% commission for TiDB Cloud ACV multiplied by the Base Rate. The Brand New Cloud Customer metric serves as the qualification threshold that must be met to activate this additional compensation incentive.\"\n",
      "    },\n",
      "    {\n",
      "      \"source_entity\": \"High-Quality OP\",\n",
      "      \"target_entity\": \"Compensation Metrics\",\n",
      "      \"type\": \"qualifies\",\n",
      "      \"definition\": \"The High-Quality OP criteria determine which TiDB Enterprise Subscriptions qualify for compensation. As stated in the compensation plan, 'Only TiDB Enterprise Subscriptions that meet the High-Quality (KA/SKA) OP (On-Opportunity) criteria will be eligible for compensation; those that do not meet these criteria will not be compensated.' This creates a qualifying relationship where the High-Quality OP metric directly impacts compensation eligibility.\"\n",
      "    },\n",
      "    {\n",
      "      \"source_entity\": \"SKA/KA Segmentation\",\n",
      "      \"target_entity\": \"High-Quality OP\",\n",
      "      \"type\": \"contributes_to\",\n",
      "      \"definition\": \"The SKA/KA Segmentation directly contributes to determining High-Quality OP status, as being classified as either a Strategic Key Account (SKA) or Key Account (KA) is one of the two possible criteria for meeting High-Quality OP requirements. This relationship establishes that customer segmentation is a pathway to qualification for compensation eligibility.\"\n",
      "    },\n",
      "    {\n",
      "      \"source_entity\": \"ATR\",\n",
      "      \"target_entity\": \"Compensation Buckets\",\n",
      "      \"type\": \"influences\",\n",
      "      \"definition\": \"The ATR (Available to Renew) metric influences the Compensation Buckets by determining how renewal deals are calculated and allocated. Specifically, 'Renewal ACV exceeding ATR will be calculated into Bucket 1' as stated in the compensation plan. Additionally, ATR defines the pool of renewable orders that can contribute to Bucket 2 quota attainment, directly affecting commission calculations.\"\n",
      "    },\n",
      "    {\n",
      "      \"source_entity\": \"Cloud First Accelerator\",\n",
      "      \"target_entity\": \"Compensation Buckets\",\n",
      "      \"type\": \"enhances\",\n",
      "      \"definition\": \"The Cloud First Accelerator enhances the Compensation Buckets structure by providing an additional 20% commission specifically for Bucket 1 (New & Expansion ACV) when the sales representative achieves their target number of Brand New Cloud Customers. This relationship represents how the accelerator program augments the standard bucket-based compensation calculation to incentivize cloud customer acquisition.\"\n",
      "    },\n",
      "    {\n",
      "      \"source_entity\": \"Compensation Metrics\",\n",
      "      \"target_entity\": \"Multi-year Orders Compensation\",\n",
      "      \"type\": \"informs\",\n",
      "      \"definition\": \"Compensation Metrics inform the Multi-year Orders Compensation structure by providing the foundational measurements and calculations upon which multi-year commission payments are based. The metrics determine how ACV is calculated, which deals qualify for compensation, and how commission rates are applied across multiple years of a contract, directly affecting the compensation outcomes for sales representatives with multi-year deals.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "from knowledge_graph.models import SourceData\n",
    "\n",
    "def get_node_attributes(node):\n",
    "    references = []\n",
    "    for child in node.children:\n",
    "        if len(child.children) > 0:\n",
    "            raise ValueError(\"Reference node should be the leaf node without any children\")\n",
    "        \n",
    "        references.append(child.name)\n",
    "    \n",
    "    return references\n",
    "\n",
    "\n",
    "def depth_traversal(root, current_path:list):\n",
    "    # print(\" \"*depth, f\"- {root.name}\")\n",
    "    if root.name == \"Reference\":\n",
    "        try:\n",
    "            references = get_node_attributes(root)\n",
    "            return [{\n",
    "                \"path\": copy.deepcopy(current_path),\n",
    "                \"references\": get_node_attributes(root),\n",
    "            }]\n",
    "        except Exception as e:\n",
    "            print(f\"Fail to get reference, skip it {e}\")\n",
    "    elif root.name == \"Definition\":\n",
    "        return [{\n",
    "            \"path\": copy.deepcopy(current_path),\n",
    "            \"definition\": get_node_attributes(root),\n",
    "        }]\n",
    "    elif root.name == \"Annotation\":\n",
    "         return [{\n",
    "            \"path\": copy.deepcopy(current_path),\n",
    "            \"annotation\": get_node_attributes(root),\n",
    "        }]\n",
    "\n",
    "\n",
    "    all_paths = []\n",
    "    for child in root.children:\n",
    "        curent_path_copy = copy.deepcopy(current_path)\n",
    "        all_paths.extend(depth_traversal(child, curent_path_copy + [root.name]))\n",
    "\n",
    "    return all_paths\n",
    "\n",
    "\n",
    "all_knowledges = {}\n",
    "for index in kb.indexes:\n",
    "    paths = depth_traversal(index, [])\n",
    "    for path in paths:\n",
    "        path_str = \"->\".join(path['path'])\n",
    "        if path_str not in all_knowledges:\n",
    "            all_knowledges[path_str] = {}\n",
    "\n",
    "        for key, value in path.items():\n",
    "            all_knowledges[path_str][key] = value\n",
    "\n",
    "all_knowledges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are an expert knowledge graph architect. Your task is to analyze the provided 'knowledge' object and its referenced document content ('reference_documents') to create concept node entities and their relationships for a knowledge graph.\n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "1.  **`knowledge` Object:** Contains the `path` (representing the topic's context or hierarchy) and `references` (names of source documents).\n",
    "    {knowledge}\n",
    "\n",
    "2.  **`reference_documents` List:** A list of objects, each containing the `id`, `name`, `link`, `version`, and `content` of the documents referenced in the `knowledge` object.\n",
    "    {reference_documents}\n",
    "\n",
    "**Task:**\n",
    "\n",
    "Based *strictly* on the information found within the `content` of the provided `reference_documents` that correspond to the `knowledge` object's topic (indicated by its `path`), generate concept node entities and their relationships.\n",
    "\n",
    "1.  **Analyze Complexity:** Evaluate the information related to the `knowledge['path']` topic within the document `content`.\n",
    "    * Consider a topic SIMPLE if: it can be fully explained in 1-2 paragraphs, has a single clear definition, and doesn't contain distinct subtopics.\n",
    "    * Consider a topic COMPLEX if: it requires extensive explanation, contains multiple aspects or dimensions, has hierarchical components, or is discussed from different perspectives in the documents.\n",
    "\n",
    "2.  **For Each Entity:**\n",
    "    * **Generate a `name`:** Create a concise, descriptive, and accurate name for the concept represented by the entity. Use terminology found in the documents or derived logically from the `knowledge['path']` and content.\n",
    "    * **Generate a `definition`:** Write a professional, clear, detailed, coherent, and logically structured definition (or description) for the entity. This definition MUST:\n",
    "        * Be derived *exclusively* from the provided `content` of the relevant `reference_documents`. Do not infer information or use external knowledge.\n",
    "        * Accurately synthesize the relevant information from the source(s).\n",
    "        * Explain the concept thoroughly.\n",
    "        * Include explanations of domain-specific terms within the definition when necessary.\n",
    "\n",
    "3.  **Information Prioritization Guidelines:**\n",
    "    * Prioritize more recent versions of documents when available.\n",
    "    * Look for consensus across multiple sources.\n",
    "    * If conflicting information exists, note the discrepancy in the definition and present the most supported view.\n",
    "\n",
    "4.  **Validation Requirements:**\n",
    "    * Ensure all key points from the source documents are represented.\n",
    "    * Verify that no information contradicts the source material.\n",
    "    * Include only information that is explicitly stated or directly implied in the source documents.\n",
    "\n",
    "5.  **Relationship Identification and Definition:**\n",
    "    * Identify meaningful relationships between entities based on the document content.\n",
    "    * For each relationship:\n",
    "        * Determine the source and target entities.\n",
    "        * Assign an appropriate relationship type that best describes the connection.\n",
    "        * Create a detailed definition explaining the nature of the relationship.\n",
    "    * Common relationship types include (but are not limited to):\n",
    "        * Hierarchical (is_part_of, contains, belongs_to)\n",
    "        * Causal (causes, results_in, depends_on)\n",
    "        * Functional (interacts_with, supports, enables)\n",
    "        * Temporal (precedes, follows, occurs_during)\n",
    "        * Comparative (is_similar_to, differs_from)\n",
    "    * Relationship definitions should:\n",
    "        * Be as detailed and professional as entity definitions\n",
    "        * Explain the specific nature of how entities interact or relate\n",
    "        * Be derived exclusively from the source documents\n",
    "        * Include directional clarity (how source affects target and vice versa)\n",
    "    * Ensure all relationships are explicitly supported by information in the source documents.\n",
    "\n",
    "6.  **Edge Case Handling:**\n",
    "    * If the referenced documents contain insufficient information:\n",
    "        * Create a minimal entity with the available information.\n",
    "        * Note in the definition that the information is limited based on the provided documents.\n",
    "    * If the topic is not clearly addressed in the documents:\n",
    "        * Create an entity based on any relevant information that can be found.\n",
    "        * Indicate areas where more information would be beneficial.\n",
    "    * If relationships are implied but not explicitly stated:\n",
    "        * Only create relationships with reasonable confidence based on the text.\n",
    "        * Note the level of certainty in the relationship definition.\n",
    "\n",
    "**Output Format:**\n",
    "\n",
    "Provide the output as a JSON object with two main sections:\n",
    "\n",
    "1. **`entities`**: A list of objects, each representing a concept node with:\n",
    "   * `name`: (String) The generated name for the entity.\n",
    "   * `definition`: (String) The generated professional definition for the entity.\n",
    "\n",
    "2. **`relationships`**: A list of objects, each representing a relationship between entities with:\n",
    "   * `source_entity`: (String) The name of the source entity.\n",
    "   * `target_entity`: (String) The name of the target entity.\n",
    "   * `type`: (String) A concise label describing the type of relationship (e.g., \"is_part_of\", \"depends_on\", \"influences\").\n",
    "   * `definition`: (String) A detailed, professional description of the relationship, explaining how the source entity relates to the target entity, based strictly on the provided document content.\n",
    "\n",
    "**Example Output Structure:**\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"entities\": [\n",
    "    {{\n",
    "      \"name\": \"Entity Name 1\",\n",
    "      \"definition\": \"Detailed, professional definition for Entity 1 based strictly on the provided document content...\"\n",
    "    }},\n",
    "    {{\n",
    "      \"name\": \"Entity Name 2\",\n",
    "      \"definition\": \"Detailed, professional definition for Entity 2 based strictly on the provided document content...\"\n",
    "    }}\n",
    "  ],\n",
    "  \"relationships\": [\n",
    "    {{\n",
    "      \"source_entity\": \"Entity Name 1\",\n",
    "      \"target_entity\": \"Entity Name 2\",\n",
    "      \"type\": \"is_component_of\",\n",
    "      \"definition\": \"Detailed explanation of how Entity 1 functions as a component of Entity 2, including specific interactions and dependencies described in the source documents...\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "```\n",
    "Remember: Quality over quantity. It's better to have fewer well-defined entities and relationships than many superficial ones. Your entities and relationships should represent distinct, meaningful concepts that would be valuable in a knowledge graph. All information must be derived exclusively from the provided documents.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_source_names = set()\n",
    "\n",
    "for path in all_knowledges.values():\n",
    "    reference_source_names.update(path.get('references', []))\n",
    "\n",
    "with SessionLocal() as db:\n",
    "    sources = db.query(SourceData).where(SourceData.name.in_(list(reference_source_names))).all()\n",
    "\n",
    "source_map = {}\n",
    "for source in sources:\n",
    "    source_map[source.name] = {\n",
    "        \"id\": source.id,\n",
    "        \"name\": source.name,\n",
    "        \"link\": source.link,\n",
    "        \"version\": source.version,\n",
    "        \"content\": source.content\n",
    "    }\n",
    "\n",
    "for knowledge in all_knowledges.values():\n",
    "    invalid_references = []\n",
    "    valid_references = []\n",
    "    if 'references' in knowledge:\n",
    "        for reference in knowledge['references']:\n",
    "            if reference not in source_map:\n",
    "                invalid_references.append(reference)\n",
    "            else:\n",
    "                valid_references.append(source_map[reference])\n",
    "    \n",
    "    if invalid_references:\n",
    "        print(f\"skip knowledge {knowledge}, caused by lack of references {invalid_references}\")\n",
    "        continue\n",
    "\n",
    "    print(valid_references)\n",
    "    print(knowledge)\n",
    "\n",
    "    input_prompt = prompt.format(knowledge=knowledge, reference_documents=valid_references)\n",
    "    response = llm_client.generate(input_prompt)\n",
    "    print(response)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "faq_file = \"docs/dataset/AI_BOT_Testing\"\n",
    "\n",
    "if os.path.exists(f\"{faq_file}.pkl\"):\n",
    "    faq_df = pd.read_pickle(f\"{faq_file}.pkl\")\n",
    "else:\n",
    "    faq_df = pd.read_excel(f\"{faq_file}.xlsx\")\n",
    "    faq_df = faq_df.iloc[2:, :5].reset_index(drop=True)\n",
    "    faq_df.columns = [\"提问者\", \"Questions\", \"AI Answers\", \"✔️ or ✖️\", \"Tree Index\"]\n",
    "    faq_df.to_pickle(faq_file)\n",
    "\n",
    "faq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(faq_df.at[1, 'Tree Index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from utils.json_utils import extract_json\n",
    "from index_craft.prompts.index_gen import get_question_index_prompt, get_index_reference_prompt\n",
    "\n",
    "issues = []\n",
    "\n",
    "for index, row in faq_df.iterrows():\n",
    "    print(type(row['Tree Index']), row['Tree Index'])\n",
    "    if not isinstance(row['Tree Index'], str) or (row['Tree Index'] is not None and len(row['Tree Index']) > 0):\n",
    "        continue\n",
    "\n",
    "    print(\"-\"*100)\n",
    "    print(\"Question: \", row['Questions'])\n",
    "    prompt = get_question_index_prompt(row['Questions'], tree_dict)\n",
    "    response = llm_client.generate(prompt)\n",
    "    json_str = extract_json(response)\n",
    "    json_obj = json.loads(json_str)\n",
    "    index_paths = []\n",
    "    for i, index_obj in enumerate(json_obj):\n",
    "        print(f\"Index {i}:\")\n",
    "        print(f\" - subquestion: {index_obj['subquestion']}\")\n",
    "        print(f\" - reasoning: {index_obj['reasoning']}\")\n",
    "        print(f\" - matched: {index_obj['matched']}\")\n",
    "        if index_obj['matched'] and 'index_path' in index_obj and len(index_obj['index_path']) > 0:\n",
    "            path = \" -> \".join(index_obj['index_path'])\n",
    "            print(\" -\", path)\n",
    "            index_paths.append(path)\n",
    "            \n",
    "        else:\n",
    "            issues.append(index_obj)\n",
    "    print(\"\\n\")\n",
    "    faq_df.loc[index, 'Tree Index'] = index_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_df.to_pickle(f\"{faq_file}.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
